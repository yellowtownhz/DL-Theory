在第三章中，分析了Deep Linear Network（Sec. \ref{sec:3-1}）中层与层之间前传的统计规律。
本章的目的是利用第三章讲到的的临界状态（criticality, Sec. \ref{sec:3}）、扰动（fluctuation, Sec. \ref{sec:3}）和第四章的Effective Theory（Sec. \ref{sec:4}），把MLP中结论推广到任意激活函数中。

\subsection{Criticality Analysis of the Kernel}
在第三章得到一个结论，在MLP中，两个输入$(x_{\alpha_1}, x_{\alpha_2})$协方差$G_{\alpha_1\alpha_2}^{(0)}$传递到$l$层的幅度为:
\begin{equation}
    G_{\alpha_1\alpha_2}^{(l)} = (C_W)^l G_{\alpha_1\alpha_2}^{(0)}.
\end{equation} 
如果我们用kernel来表示G，并且考虑bias，第$l$层和第$l+1$层之间的传递公式为：
\begin{equation}
    K_{\alpha\beta}^{(l+1)} = C_b + C_W\langle\sigma_\alpha \sigma_\beta\rangle_{K^{(l)}}
    \label{eq:kernel_recursion}
\end{equation} 
在第一层的初始状态为：
\begin{equation}
    K_{\alpha\beta}^{(1)} = C_b + C_W(\frac{1}{n_0}\sum^{n_0}_{i=1}x_{i;\alpha}x_{j;\beta})
\end{equation}
$K_{\alpha\beta}$的统计含义就是两个输入$\alpha$和$\beta$之间的协方差
我们的目的就是分析$K_{\alpha\beta}$如何随着层数的加深而不断变化的。

\subsubsection{Single Input}
首先我们考虑只有一个输入样本的情况，定义
\begin{equation}
    \begin{aligned}
    K_{00}^{(l+1)} &= C_b + C_W\langle\sigma(z_0)\sigma(z_0)\rangle_{K^{(l)}} \\
    &= C_b + C_Wg(K_{00}^{(l)})
    \end{aligned}
\label{eq:4-1-1}
\end{equation}
其中$g$为定义的一个辅助函数：
\begin{equation}
    g(K)\equiv \langle\sigma(z)\sigma(z)\rangle_{K} \equiv \frac{1}{\sqrt{2\pi K}}\int^{\inf}_{\inf}dz e^{-\frac{z^2}{2K}}\sigma(z)\sigma(z),
\end{equation}
第$l$层kernel的定义为：
\begin{equation}
    K_{00}^{(l)} = \mathbb{E}[\frac{1}{n_{l}}\sum^{n_{l}}_{i=1}(z^{(l)}_{i;0})^2]
\end{equation}

我们研究的是一个``固定点''$K_{00}^{*}$，使得Eq. \ref{eq:4-1-1}在这个固定点处，输出等于输入，即：
\begin{equation}
    K_{00}^{*} = C_b + C_Wg(K_{00}^{*})
\end{equation}
在$K_{00}^{*}$处微分化：
\begin{equation}
    K_{00}^{(l)} = K_{00}^{*} + \delta K_{00}^{l}
\end{equation}
\begin{equation}
    \delta K_{00}^{(l+1)} = \chi_\parallel(K_{00}^{*})\delta K_{00}^{*} + O(\delta^2)
\label{eq:4-1-2}
\end{equation}
其中，$\chi_\parallel(K_{00}^{*})$被称作parallel susceptibility。当$\chi_\parallel(K_{00}^{*})<1$时，特征的协方差会随着逐层前传而逐渐减小，直至消失；当$\chi_\parallel(K_{00}^{*})>1$时，特征协方差会逐渐变大，直至爆炸。所以只有\textbf{临界条件}$\chi_\parallel(K_{00}^{*})=1$满足的时候，特征的协方差才会稳定。

\subsubsection{Two Inputs}
当有两个样本的时候，Eq. \ref{eq:4-1-2}变得更复杂：
\begin{equation}
    \delta \delta K_{[2]}^{(l+1)} = \chi_\perp(K_{00}^{(l)})\delta \delta K_{[2]}^{l} + h(K_{00}^{(l)})(\delta K_{[1]}^{(l)})^2
\label{eq:4-1-3}
\end{equation}
此时，临界条件变为：
\begin{equation}
    \chi_\parallel(K_{00}^{*})=1, \qquad \chi_\perp(K_{00}^{*})=1
\label{eq:4-1-4}
\end{equation}

\subsection{Criticality for Scale-Invariant Activations}
回忆第二章中对scale-invariant函数的定义：
\begin{equation}
    \sigma(z)=\left\{
        \begin{aligned}
            a_{+}z, z\geq 0,\\
            a_{-}z, z< 0.
        \end{aligned}
    \right.
\end{equation}
在scale-invariant函数的条件下，helper function $g$变得非常简单：
\begin{equation}
    g(K)=A_2 K, \qquad A_2\equiv \frac{a_{+}^2+a_{-}^2}{2}
\end{equation}
数学推导可得，这种条件下两个susceptibility相等，并且和$K_{00}^{(l)}$独立：
\begin{equation}
    \chi_\parallel(K_{00}^{(l)})=\chi_\perp(K_{00}^{(l)})=A_2 C_W \equiv \chi
\end{equation}
当$\chi>1$时，特征幅度会逐渐爆炸；当$\chi<1$时，特征幅度会逐渐消失；只有当$C_W=1/A_2, C_b=0$时，状态为临界状态，此时：
\begin{equation}
    K_{00}^{*}=\frac{1}{A_2}(\frac{1}{n_0}\sum^{n_0}_{i=1}x_{i;0}^2)
\end{equation}

总而言之，deep linear network的临界条件为：
\begin{equation}
    (C_b, C_W)^{critical}=(0, \frac{1}{A_2}), \qquad A_2=(a_{+}^2+a_{-}^2)/2
\end{equation}
当激活函数为$\mathtt{ReLU}$时，这个形式就等同于Kaiming初始化$(C_b, C_W)^{critical}=(0, 2)$

\subsection{Universality beyond Scale-Invariant Activations}
本小节简单推导其他激活函数有没有临界状态，有的话临界状态需要满足哪些条件。

从上一小节我们可以得出判断临界状态的两步走：
\begin{itemize}
    \item 第一步，对于每一对$C_b$和$C_W$，找到临界点$K_{00}^*=K_{00}^*(C_b, C_W)$，使得$K_{00}^*=C_b+C_W g_0(K_{00}^*)$，同时$K_{00}^*\ge0$
    \item 第二步，对于找到的每一个临界点$K_{00}^*=K_{00}^*(C_b, C_W)$，计算$\chi_\parallel(K_{00}^*)$和$\chi_\perp(K_{00}^*)$，寻找满足临界条件$\chi_\parallel(K_{00}^{*})=1, \qquad \chi_\perp(K_{00}^{*})=1$的$(C_b, C_W)$
\end{itemize}

\subsubsection{$\texttt{tanh}$}
\begin{figure}[!ht]
    \includegraphics[width=\textwidth]{images/4_tanh.bmp}
    \caption{
        两种计算$\mathtt{tanh}$函数临界条件的算法。\textbf{左图}：横坐标表示$C_W$，纵坐标表示$C_b$，实线表示的是满足$\chi_\perp(K_{00}^{*})=1$的$(C_b, C_W)$值，虚线表示的是满足$\chi_\parallel(K_{00}^{*})=1$的$(C_b, C_W)$值。两条线相交在$(C_b, C_W)=(1,0)$处。\textbf{右图}：$LHS$曲线，可以看到满足$LHS=1$时，$K_{00}^*\to0$
    }
    \label{fig:4-tanh}
\end{figure}
有图\ref{fig:4-tanh}左图可以看到，$\texttt{tanh}$的临界状态为$(C_b, C_W)=(1,0)$。

在实际使用中，上述算法计算过于繁琐，所以改为第二种更容易操作算法：
\begin{itemize}
    \item 第一步，搜索$K_{00}^{*}\ge1$，找出满足$LHS\equiv\left[\frac{2K^2 \langle\sigma^{'}(z)\sigma^{'}(z)\rangle_K}{\langle\sigma(z)\sigma(z)(z^2-K)\rangle_K}\right]\arrowvert_{K=K_{00}^*}$的$K_{00}^{*}$
    \item 第二步，把找到的$K_{00}^{*}$代入公式$C_W=[\langle\sigma^{'}(z)\sigma^{'}(z)\rangle_{K_{00}^*}]^{-1}$和$C_b=K_{00}^*-\frac{\langle\sigma(z)\sigma(z)\rangle_{K_{00}^*}}{\langle\sigma^{'}(z)\sigma^{'}(z)\rangle_{K_{00}^*}}$，得到满足临界条件的初始化参数（保证$C_b\ge0, C_W\ge 0$）。
\end{itemize}
在图\ref{fig:4-tanh}右图中可以看到，满足临界条件时，$K_{00}^*\to0$，代入公式可得：
\begin{equation}
    C_W=\left[\langle\sigma^{'}(z)\sigma^{'}(z)\rangle_{K_{00}^*}\right]^{-1}=(\sigma^{'}(0))^{-2}=1,
\end{equation}
\begin{equation} 
    C_b=K_{00}^*-\frac{\langle\sigma(z)\sigma(z)\rangle_{K_{00}^*}}{\langle\sigma^{'}(z)\sigma^{'}(z)\rangle_{K_{00}^*}}=-\left(\frac{\sigma(0)}{\sigma^{'}(0)}\right)^2=0
\end{equation}

\subsubsection{$\texttt{sigmoid}$}
\begin{figure}[!ht]
    \includegraphics[width=\textwidth]{images/4_sigmoid_softplus.bmp}
    \caption{
        $\texttt{sigmoid}$和$\texttt{softplus}$的$LHS$曲线。
    }
    \label{fig:4-sigmoid}
\end{figure}
$\texttt{sigmoid}$的公式为：
\begin{equation}
    \sigma(z)=\frac{1}{1+\exp^{-z}}
\end{equation}
由图\ref{fig:4-sigmoid}可以看到，$\texttt{sigmoid}$在$K_{00}^*=0$时，$LHS=0$，满足临界条件。但是此时$C_b=-\left(\frac{\sigma(0)}{\sigma^{'}(0)}\right)^2$，所以$\texttt{sigmoid}$函数没有临界状态。

\subsubsection{$\texttt{softplus}$}
\begin{figure}[!ht]
    \centering
    \begin{subfigure}
    {
        \includegraphics[width=0.4\textwidth]{images/4_softplus_func.bmp}
        \label{fig:4-softplus-func}
    }
    \end{subfigure}
    \begin{subfigure}
    {
        \includegraphics[width=0.5\textwidth]{images/4_softplus_LHS.bmp}
        \label{fig:4-softplus-LHS}
    }
    \end{subfigure}
    \caption{$\texttt{softplus}$的函数曲线$LHS$曲线}
\end{figure}

$\texttt{softplus}$的计算公式为：
\begin{equation}
    \sigma(z)=log(1+e^z),
\end{equation}
函数曲线为图\ref{fig:4-softplus-func}。从图\ref{fig:4-softplus-LHS}可以看到，$\texttt{softplus}$的$LHS$曲线没有等于1的情况，所以尽管$\texttt{softplus}$函数从样子上属于$\texttt{ReLU}$的平滑版本，但是依旧没有临界状态。

\subsubsection{$\texttt{SWISH}$ and $\texttt{GELU}$}
分析略去，结论：

如果要使用平滑版本的$\texttt{ReLU}$函数，$\texttt{SWISH}$和$\texttt{GELU}$都不是一个好的选择，应该使用$\texttt{tanh}$。

\subsection{Fluctuations}
主要结论和Sec. \ref{sec:3-3}一致：
\begin{itemize}
    \item 在deep linear networks中，有限宽网络带来的方差抖动（finite-width fluctuations）会随着depth-to-width的比值增大而增大。这个结论在和非线性激活函数的网络同样适用。
    \item 在$L/n$固定，$n,L\to \infty$的情况下，绝大部分足够宽、不是太深的网络都可以不用考虑NLO metric $G_{\alpha_1\alpha2}^{\{1\}\{l\}}$的作用
\end{itemize}