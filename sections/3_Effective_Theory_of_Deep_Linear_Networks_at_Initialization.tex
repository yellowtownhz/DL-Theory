这一章主要从线性神经网络进行推导，相当于是激活函数为线性函数的MLP网络。

\subsection{Deep Linear Networks}

线性神经网络中，$l+1$层第$i$个原子与$l$层原子的迭代关系如下:
\begin{equation}
    z_{i;\alpha}^{(l+1)} = b_i^{(l+1)} + \sum_{j=1}^{n_l} W_{ij}^{l+1}z_{j;\alpha}^{(l)},
\end{equation}
这里$\alpha$用来表示不同的输入样本。实际上可以直接从输入$x$到第$l$层进行合并，变成一个矩阵相乘的形式:
\begin{equation}
    z_{i;\alpha}^{(l)} = \sum_{j_0=1}^{n_0}\sum_{j_1=1}^{n_1} \cdots \sum_{j_{l-1}=1}^{n_{l-1}}
                W_{ij_{l-1}}^{(l)} W_{j_{l-1}j_{l-2}}^{(l-1)} \cdots W_{j_1j_0}^{(1)} x_{j_0;\alpha}
              = \sum_{j=1}^{n_0} \mathcal{W}_{ij}^{(l)} x_{j;\alpha}.
\end{equation}

Here $\mathcal{W}$ is an $n_l$-by-$n_0$ matrix:
\begin{equation}
    \mathcal{W}_{ij}^{(l)} = \sum_{j_0=1}^{n_0}\sum_{j_1=1}^{n_1} \cdots \sum_{j_{l-1}=1}^{n_{l-1}}
                W_{ij_{l-1}}^{(l)} W_{j_{l-1}j_{l-2}}^{(l-1)} \cdots W_{j_1j}^{(1)}.
\end{equation}

设定每一层权重的协方差是固定的（$C_W^{(l)} = C_W$）且为零均值，参数之间相互独立:
\begin{equation}
    \mathbb{E}[W_{ij}^{(l)} = 0], ~~ \mathbb{E}[W_{i_1j_1}^{(l)}W_{i_2j_2}^{(l)}] 
                                    = \delta_{i_1i_2}\delta_{j_1j_2}\frac{C_W}{n_{l-1}}
    \label{eq:expectation}
\end{equation}
(\xu{why $n_{l-1}$ here?}) 这里$\delta_{ij} = 1 ~ if ~ i=j, else ~ \delta_{ij} = 0$.

要研究$z^{(l)}$的分布问题，按照高斯分布的研究方式的话，主要关注$z^{(l)}$的均值和方差。均值推导如下:
\begin{equation}
    \begin{aligned}
    \mathbb{E}[z_{i;\alpha}^{(l)}] =& \mathbb{E}[\sum_{j=1}^{n_0} \mathcal{W}_{ij}^{(l)} x_{j;\alpha}] \\
     =& \sum_{j_0=1}^{n_0}\sum_{j_1=1}^{n_1} \cdots \sum_{j_{l-1}=1}^{n_{l-1}}
       \mathbb{E}[W_{ij_{l-1}}^{(l)}] \mathbb{E}[W_{j_{l-1}j_{l-2}}^{(l-1)}]
       \cdots \mathbb{E}[W_{j_1j_0}^{(1)}] x_{j_0;\alpha} = 0
    \end{aligned}
\end{equation}
这里用到了层与层之间参数相互独立的假设。接下来就是要研究$z^{(l)}$的协方差分布的问题。

\subsection{Criticality [物理学临界状态]}

\subsubsection{数学推导}
这一节首先研究一下两个原子之间的协方差$\mathbb{E}[z_{i_1;\alpha_1}^{(l)}z_{i_2;\alpha_2}^{(l)}]$，
因为协方差是否为$0$决定了各原子分布是否为高斯分布（相互独立的服从高斯分布的变量的和也为高斯分布，如果不独立，分布将变得难以分析）。

\begin{equation*}
    \begin{aligned}
   \mathbb{E} [z_{i_1;\alpha_1}^{(1)}z_{i_2;\alpha_2}^{(1)}] =& \sum_{j_1,j_2=1}^{n_0} 
            \mathbb{E} [W_{i_1j_1}^{(1)}x_{j_1;\alpha_1}W_{i_2j_2}^{(1)}x_{j_2;\alpha_2}] \\
    =& \sum_{j_1,j_2=1}^{n_0} \mathbb{E} [W_{i_1j_1}^{(1)}W_{i_2j_2}^{(1)}]x_{j_1;\alpha_1}x_{j_2;\alpha_2},
    \end{aligned}
\end{equation*}
代入公式 \ref{eq:expectation}，然后把第二个$\delta_{j_1j_2}$合并一下, 可以简化为:
\begin{equation*}
             = \sum_{j_1,j_2=1}^{n_0} \frac{C_W}{n_0} 
             \delta_{i_1i_2}\delta_{j_1j_2}x_{j_1;\alpha_1}x_{j_2;\alpha_2}
             = \delta_{i_1i_2}C_W \frac{1}{n_0}\sum_{j=1}^{n_0}x_{j;\alpha_1}x_{j;\alpha_2}.
\end{equation*}
这里稍微用到了维克定理的做法。
类似地，可以得到从$l$到第$l+1$的递推公式:
\begin{equation}
    \mathbb{E}[z_{i_1;\alpha_1}^{(l+1)} z_{i_2;\alpha_2}^{(l+1)}] = 
        \delta_{i_1i_2}C_W \frac{1}{n_l}\sum_{j=1}^{n_l}\mathbb{E} 
            [z_{j;\alpha_1}^{(l)}z_{j;\alpha_2}^{(l)}].
    \label{eq:l-2-lp1}
\end{equation}
对任意两个由输入维度归一化之后的内积定义如下:
\begin{equation*}
    G_{\alpha_1\alpha_2}^{(0)} \equiv \frac{1}{n_0} \sum_{i=1}^{n_0} x_{i;\alpha_1}x_{i;\alpha_2}, ~~
    G_{\alpha_1\alpha_2}^{(l)} \equiv \frac{1}{n_l} \sum_{j=1}^{n_l} 
        \mathbb{E}[z_{j;\alpha_1}^{(l)}z_{j;\alpha_2}^{(l)}],
    \label{eq:2corr-def}
\end{equation*}
根据这个定义以及公式\ref{eq:l-2-lp1}，可以把从$0$到$1$以及从$l$到$l+1$递归地写成:
\begin{equation}
    \mathbb{E}[z_{i_1;\alpha_1}^{(1)}z_{i_2;\alpha_2}^{(1)}] = \delta_{i_1i_2}C_W G_{\alpha_1\alpha_2}^{(0)},
\end{equation}
\begin{equation}
    G_{\alpha_1\alpha_2}^{(l+1)} = 
        \frac{1}{n_{l+1}} \sum_{j=1}^{n_{l+1}} \mathbb{E}[z_{j;\alpha_1}^{(l+1)}z_{j;\alpha_2}^{(l+1)}] 
        = \frac{1}{n_{l+1}}\sum_{j=1}^{n_{l+1}} \delta_{jj}C_W G_{\alpha_1\alpha_2}^{(l)}
        = C_W G_{\alpha_1\alpha_2}^{(l)},
\end{equation}
由此，可以得到由两个输入$(x_{\alpha_1}, x_{\alpha_2})$协方差$G_{\alpha_1\alpha_2}^{(0)}$传递到第
$l$层的幅度为:
\begin{equation}
    G_{\alpha_1\alpha_2}^{(l)} = (C_W)^l G_{\alpha_1\alpha_2}^{(0)}.
    \label{eq:recursion}
\end{equation} 

\subsubsection{物理学解释}
根据迭代公式 \ref{eq:recursion}, 当$C_W > 1$, 随着层数$l$递增，$G_{\alpha_1\alpha_2} \to \infty$ (exploding,
numerical instability); 当 $C_W < 1$, $G_{\alpha_1\alpha_2} \to 0$ (valishing, loss of information), 
原文中称之为``trivial fixed point''. 这里是从协方差传递的角度进行分析， 
\emph{神经网络初始化策略研究中，有类似的结论，通过研究单个原子的均值和方差随层数增加的变化，提出了xavier初始化策略}.
(\xu{action: 附上reference}). 当然，第$L$层的原子的方差/幅度也可以通过如下公式得到（均值为$0$）:
\begin{equation*}
    G_{\alpha\alpha}^L = \mathbb{E} [\frac{1}{n_L} \sum_{j=1}^{n_L}(z_{j;\alpha}^{(L)})^2].
\end{equation*}
接上文，当$C_W=1$的时候，各层的协方差都是固定的 
$G_{\alpha_1\alpha_2}^{(l)} = G_{\alpha_1\alpha_2}^{(0)} \equiv G_{\alpha_1\alpha_2}^*$. 这里
协方差保持一致的意义在于，输入数据的结构信息(structure)在网络传递的过程中得到了保留。

\subsection{Fluctuations [物理学中的扰动]}
上一节研究了$2$个原子之间的协方差关系，为了进行扩展，接下来研究$4$个原子的协方差关系($4$-point correlator)，
基本思路是拆分为$2$个原子之间协方差的组合。
跟公式 \ref{eq:2corr-def} 一样的定义，$4$阶归一化内积可以定义为：
\begin{equation}
    G_4^{(l)} = \frac{1}{n_l^2} \sum_{j,k=1}^{n_l} 
        \mathbb{E}[z_j^{(l)}z_j^{(l)}z_k^{(l)}z_k^{(l)}].
\end{equation}
从$l$层到$l+1$层的$4$阶correlator可以进行如下推导:
\begin{equation}
    \begin{aligned}
        & \mathbb{E}[z_{i_1}^{(l+1)}z_{i_2}^{(l+1)}z_{i_3}^{(l+1)}z_{i_4}^{(l+1)}] \\
        =& \sum_{j_i,j_2,j_3,j_4=1}^{n_l} 
            \mathbb{E}[W_{i_1j_1}^{(l+1)}W_{i_2j_2}^{(l+1)} W_{i_3j_3}^{(l+1)}W_{i_4j_4}^{(l+1)}] 
            \mathbb{E}[z_{j_1}^{(l)}z_{j_2}^{(l)} z_{j_3}^{(l)}z_{j_4}^{(l)}] \\
        =& \frac{C_W^2}{n_l^2} \sum_{j_i,j_2,j_3,j_4=1}^{n_l} 
            [(\delta_{i_1i_2}\delta_{j_1j_2}\delta_{i_3i_4}\delta_{j_3j_4}
            +\delta_{i_1i_3}\delta_{j_1j_3}\delta_{i_2i_4}\delta_{j_2j_4}
            +\delta_{i_1i_4}\delta_{j_1j_4}\delta_{i_2i_3}\delta_{j_2j_3})] \\
        & \times \mathbb{E}[z_{j_1}^{(l)}z_{j_2}^{(l)}z_{j_3}^{(l)}z_{j_4}^{(l)}] \\
        = & C_W^2 (\delta_{i_1i_2}\delta_{i_3i_4} + \delta_{i_1i_3}\delta_{i_2i_4} 
            + \delta_{i_1i_4}\delta_{i_2i_3}) \frac{1}{n_l^2} 
            \sum_{j,k=1}^{n_l}\mathbb{E}[z_j^{(l)}z_j^{(l)}z_k^{(l)}z_k^{(l)}] \\
        = & C_W^2 (\delta_{i_1i_2}\delta_{i_3i_4} + \delta_{i_1i_3}\delta_{i_2i_4} 
            + \delta_{i_1i_4}\delta_{i_2i_3}) G_4^{(l)},
    \end{aligned}
    \label{eq:4point-recur}
\end{equation}
由此可得,
\begin{equation}
    \begin{aligned}
    G_4^{(l+1)} = & \frac{1}{n_{l+1}^2} \sum_{i,j=1}^{n_{l+1}}
        \mathbb{E}[z_i^{(l+1)}z_i^{(l+1)}z_j^{(l+1)}z_j^{(l+1)}] \\
        = & \frac{1}{n_{l+1}^2} C_W^2 \sum_{i,j=1}^{n_{l+1}}
            (\delta_{ii}\delta_{jj} + \delta_{ij}\delta_{ij} + \delta_{ij}\delta_{ji}) G_4^{(l)} 
        = C_W^2 (1 + \frac{2}{n_{l+1}}) G_4^{(l)}.
    \end{aligned}
\end{equation}
根据公式\ref{eq:4point-recur}, 从$0$到$1$层的推导类似:
\begin{equation*}
    \begin{aligned}
     \mathbb{E}[z_{i_1}^{(1)}z_{i_2}^{(1)}z_{i_3}^{(1)}z_{i_4}^{(1)}]
    =& C_W^2 (\delta_{i_1i_2}\delta_{i_3i_4} + \delta_{i_1i_3}\delta_{i_2i_4} 
            + \delta_{i_1i_4}\delta_{i_2i_3}) \frac{1}{n_0^2} 
            \sum_{j,k=1}^{n_0} \mathbb{E}[x_jx_jx_kx_k] \\
    =& C_W^2 (\delta_{i_1i_2}\delta_{i_3i_4} + \delta_{i_1i_3}\delta_{i_2i_4} 
            + \delta_{i_1i_4}\delta_{i_2i_3}) \frac{1}{n_0^2} 
            \sum_{j=1}^{n_0} \mathbb{E}[x_jx_j] \sum_{k=1}^{n_0} \mathbb{E}[x_k x_k] \\
    =&  C_W^2 (\delta_{i_1i_2}\delta_{i_3i_4} + \delta_{i_1i_3}\delta_{i_2i_4} 
            + \delta_{i_1i_4}\delta_{i_2i_3}) (G_2^{(0)})^2
    \end{aligned}
\end{equation*}
这里用到了同一条数据不同维度$x_j$和$x_k$互不相关的假定。同样地，可以得到第一层$4$-point correlator为:
\begin{equation}
    G_4^{(1)} = C_W^2 (1+ \frac{2}{n_1}) (G_2^{(0)})^2
    \label{eq:4point-0}
\end{equation}
综合公式\ref{eq:4point-0}和公式\ref{eq:4point-recur}, 可以得到:
\begin{equation}
    G_4^{(l)} = C_W^{2l}[\prod_{l'=1}^l(1+\frac{2}{n_{l'}})](G_2^{(0)})^2
        = \prod_{l'=1}^l(1+\frac{2}{n_{l'}})(G_2^{(l)})^2
\end{equation}
第二个等号用到了公式\ref{eq:recursion}的结果。

\xu{TODO: 补充analysis.}