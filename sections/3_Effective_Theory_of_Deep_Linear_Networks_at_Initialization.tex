这一章主要从线性神经网络进行推导，相当于是激活函数为线性函数的MLP网络。

\subsection{Deep Linear Networks}

线性神经网络中，$l+1$层第$i$个原子与$l$层原子的迭代关系如下:
\begin{equation}
    z_{i}^{(l+1)} = b_i^{(l+1)} + \sum_{j=1}^{n_l} W_{ij}^{l+1}z_{j}^{(l)},
\end{equation}
这里形式比较直观，就不做过多解释了。实际上可以直接从输入$x$到第$l$层进行合并，变成一个矩阵相乘的形式:
\begin{equation}
    z_i^{(l)} = \sum_{j_0=1}^{n_0}\sum_{j_1=1}^{n_1} \cdots \sum_{j_{l-1}=1}^{n_{l-1}}
                W_{ij_{l-1}}^{(l)} W_{j_{l-1}j_{l-2}}^{(l-1)} \cdots W_{j_1j_0}^{(1)} x_{j_0}
              = \sum_{j=1}^{n_0} \mathcal{W}_{ij}^{(l)} x_j.
\end{equation}

Here $\mathcal{W}$ is an $n_l$-by-$n_0$ matrix:
\begin{equation}
    \mathcal{W}_{ij}^{(l)} = \sum_{j_0=1}^{n_0}\sum_{j_1=1}^{n_1} \cdots \sum_{j_{l-1}=1}^{n_{l-1}}
                W_{ij_{l-1}}^{(l)} W_{j_{l-1}j_{l-2}}^{(l-1)} \cdots W_{j_1j}^{(1)}.
\end{equation}

设定每一层权重的协方差是固定的（$C_W^{(l)} = C_W$）且为零均值，层与层之间参数相互独立：
\begin{equation}
   \mathbb{E}[W_{ij}^{(l)} = 0], ~~ \mathbb{E}[W_{i_1j_1}^{(l)}W_{i_2j_2}^{(l)}] 
                                    = \delta_{i_1i_2}\delta_{j_1j_2}\frac{C_W}{n_{l-1}}
\end{equation}
\xu{why $n_{l-1}$ here?}

要研究$z^{(l)}$的分布问题，按照高斯分布的研究方式的话，主要关注$z^{(l)}$的均值和方差。均值推导如下：
\begin{equation}
    \mathbb{E}[z_i^{(l)}] = \mathbb{E}[\sum_{j=1}^{n_0} \mathcal{W}_{ij}^{(l)} x_j]
     = \sum_{j_0=1}^{n_0}\sum_{j_1=1}^{n_1} \cdots \sum_{j_{l-1}=1}^{n_{l-1}}
       \mathbb{E}[W_{ij_{l-1}}^{(l)}] \mathbb{E}[W_{j_{l-1}j_{l-2}}^{(l-1)}]
       \cdots \mathbb{E}[W_{j_1j_0}^{(1)}] x_{j_0} = 0
\end{equation}
接下来就是要研究$z^{(l)}$的协方差分布的问题。

\subsection{Criticality}
